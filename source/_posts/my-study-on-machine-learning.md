---
title: 谈谈我和机器学习
date: 2016-10-17 00:46:50
tags: 机器学习
---
2011年大学毕业，弃了两个公司的Offer，打算去外面看看，因为觉得自己对计算机的学术研究还有一点热情。很幸运的去了台湾一所知名高校，当时想做很奇特的人机交互——通过人的手势给机器下达指令。但是不知道是什么原理，如何去实现，于是就选择了机器人实验室（这个实验室也不是专门做机器学习研究的，更多的是应用机器学习的方法），作为自己的硕士研究方向。

大学的毕设做了一个自己也不是很明白的课题——通过进化算法优化反应面的问题。什么是进化算法，什么是反应面，什么是优化？对这些前提的理解在当时只是停留在字面的解释。写到这里，发现坑到了自己，因为目前我也无法用通俗的语言来描述这个进化算法；而反应面方法，是在统计学里的一个专门术语，对应y=f(x)中的y值（当x为2维及以上的时候，整个函数的图像就像一个面）；优化是什么？它不是让程序运行更快，而是一个最优值的求解问题。什么是最优值，就是模型的预测结果和真实的观察值之间差距最小的值。

第一个涉及人机交互的算法是**Dynamic Time Wraping(DTW)**. 那时需要识别一些简单的手势，例如画圈，画横线，画竖线这些，用于机器人的实时控制。当时学长学姐推荐我用这个算法，于是我就自己录了一些样本(自己比划动作，并且标记动作名称)，大概一个动作录10到20组样本吧。把这些数据存入机器人，相当于是让它学习了这些动作。然后实际操作时，机器人通过摄像头(我们采用kinect捕捉手臂的位置并且在三维坐标系中标记坐标位置)捕捉手的运动轨迹，然后与之前的样本比对，通过DTW找出与当前动作最相似的动作。当时就觉得机器学习不过如此。

后来上了一门课，叫做**Convex Optimization**. 翻译成中文叫做凸优化。又是优化，突然联想到了大学毕设的反应面优化的问题。当时并不知道这门课是干什么的，只是上课的老师特别有名，他开发了libsvm. 是机器学习中使用最普遍的工具。一开始的上课内容，就是求解各种凸函数，比如已知一个凸函数y=f(x), 并且x通常有一些限制g(x)<=0，求出另y最小的那一个，或者那一些x.  然后讲近似，拟合。反正到后面是听不懂在讲什么了，只知道就是用各种方法求解一个参数矩阵，比如在线性规划中f(x) = wx, 给定一些已知的x和y, 求解w，使得f(x) 与 y的差值最小，就是求解一个最优化问题。到了学期快结束的时候，就厉害了，老师终于亮出了大招，利用之前所学的知识，推导了一遍Support Vector Machine(SVM). 当时就觉得自己也很厉害，虽然大半学期的课没听懂，但是也能推导出SVM. 那时觉得机器学习就是一个分类问题，在两类样本中，找出一个平面，把这些样本分为两群，而我们所要做的，就是求解矩阵，找出这个平面的参数。

在台湾念硕士只要2年，很快到了硕二，毕业论文也提上了日程。自己的研究方向是识别人的动作，人的动作是动态的，连续的，后一个动作是跟前一个动作相关的。需要有个模型来建立前后动作之间的关系，也就是建立视频中前一帧和后一帧之间的关系。于是接触到了**Hidden Markov Model(HMM)** 和 **Conditional Random Field(CRF)** 这两个图模型，这两个模型很相似，但是HMM只需要计算前后两个状态之间的转换概率，而CRF是计算在一些前提下的，前后两个状态之间转换的条件概率。我大学时的概率论学的也不怎样，在研究这两个模型之间，花了不少功夫。那时机器学习对我来讲，就是一堆状态的转移，概率的计算。然而导师也是一个牛逼的人物（IEEE Fellow）,对我的硕士论文要求也很高，他说要把自己当成是爱因斯坦那样的人物来做事情，换句话讲就是写个随随便便的论文在他那里肯定是不能毕业的。于是我在无数次打击之中，终于提出了令他满意，也坑了自己的题目——同时物体识别与动作识别。所以机器学习只是一个方法，如何去解决现实中的问题，也是非常重要的。

在处理机器学习的问题上，如何收集样本，如何提取特征，是一个非常麻烦的问题。比如很常见的逻辑回归(Logistic Regression)，当有很好的样本，和很好的特征的时候，它的效果会变得很好。但是如果学习样本取的不合适（比如正样本和负样本的比例不均衡，不同特征之间关联性很高），最终会导致模型歪了，学习出来也没有什么应用价值。

后来接触了深度学习（Deep learning），深度学习，神经网络和机器学习之间又是什么关系？首先从神经网络开始，这是一个机器学习的分支，就是模拟人的神经元，构成一个网络，把输入层的信号，通过隐藏层，映射到输出层。由于神经网络的学习方式，导致了中间的隐藏层不能太深，太深了会导致模型出问题，就是说机器学不到什么东西。后来有人改进了神经网络的学习方式，从而使得隐藏层变得很多的时候，学习也不这么难了。在深度学习中，常见的CNN和RNN都是很好的模型。CNN在图像识别中有很广泛的应用（Alpha go在识别棋局的时候，也用到了CNN）。而RNN特别适合应用在有上下文关系的场景中，其实有点类似HMM.

深度学习的最大特点就是不用特别关心如何做特征工程，比如CNN去识别物体，不用自己设计特征（比如原始图像识别中常用的HOG, SIFT特征），用最底层的像素作为特征的CNN，在有大量的样本的前提下，设定合适的神经元和层数，就能做出非常有效的模型。比如识别手写数字（MINST数据集），识别率能达到97%以上。当然，如果再加上一些学习前的预处理（比如物体集中在画图像中间），效果会更加好。

目前离开学校也好多年了，对于模型上的研究也越来越少，工作中的机器学习，更多的是如何去收集数据，清洗数据，提取样本，选择特征，选择模型和实验评估。在互联网公司，用户和系统的交互都会以日志的形式记录下来，机器学习的难点，已经转移到了对于数据的处理和分析。